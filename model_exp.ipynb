{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Layer\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Convolution1D, Merge, Dropout, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.merge import concatenate, Concatenate\n",
    "from load_glove_embeddings import load_glove_embeddings\n",
    "import re\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'news20/20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 400001\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, embedding_matrix = load_glove_embeddings(GLOVE_DIR + '/glove.6B.50d.txt', embedding_dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive',\n",
       " 'name',\n",
       " 'atheism',\n",
       " 'resources',\n",
       " 'alt',\n",
       " 'atheism',\n",
       " 'archive',\n",
       " 'name',\n",
       " 'resources',\n",
       " 'last',\n",
       " 'modified',\n",
       " '11',\n",
       " 'december',\n",
       " '1992',\n",
       " 'version',\n",
       " '1',\n",
       " '0',\n",
       " 'atheist',\n",
       " 'resources',\n",
       " 'addresses',\n",
       " 'of',\n",
       " 'atheist',\n",
       " 'organizations',\n",
       " 'usa',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'religion',\n",
       " 'foundation',\n",
       " 'darwin',\n",
       " 'fish',\n",
       " 'bumper',\n",
       " 'stickers',\n",
       " 'and',\n",
       " 'assorted',\n",
       " 'other',\n",
       " 'atheist',\n",
       " 'paraphernalia',\n",
       " 'are',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'from',\n",
       " 'religion',\n",
       " 'foundation',\n",
       " 'in',\n",
       " 'the',\n",
       " 'us',\n",
       " 'write',\n",
       " 'to',\n",
       " 'ffrf',\n",
       " 'p',\n",
       " 'o',\n",
       " 'box',\n",
       " '750',\n",
       " 'madison',\n",
       " 'wi',\n",
       " '53701',\n",
       " 'telephone',\n",
       " '608',\n",
       " '256',\n",
       " '8900',\n",
       " 'evolution',\n",
       " 'designs',\n",
       " 'evolution',\n",
       " 'designs',\n",
       " 'sell',\n",
       " 'the',\n",
       " 'darwin',\n",
       " 'fish',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'fish',\n",
       " 'symbol',\n",
       " 'like',\n",
       " 'the',\n",
       " 'ones',\n",
       " 'christians',\n",
       " 'stick',\n",
       " 'on',\n",
       " 'their',\n",
       " 'cars',\n",
       " 'but',\n",
       " 'with',\n",
       " 'feet',\n",
       " 'and',\n",
       " 'the',\n",
       " 'word',\n",
       " 'darwin',\n",
       " 'written',\n",
       " 'inside',\n",
       " 'the',\n",
       " 'deluxe',\n",
       " 'moulded',\n",
       " '3d',\n",
       " 'plastic',\n",
       " 'fish',\n",
       " 'is',\n",
       " '4',\n",
       " '95',\n",
       " 'postpaid',\n",
       " 'in',\n",
       " 'the',\n",
       " 'us',\n",
       " 'write',\n",
       " 'to',\n",
       " 'evolution',\n",
       " 'designs',\n",
       " '7119',\n",
       " 'laurel',\n",
       " 'canyon',\n",
       " '4',\n",
       " 'north',\n",
       " 'hollywood',\n",
       " 'ca',\n",
       " '91605',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'bay',\n",
       " 'area',\n",
       " 'can',\n",
       " 'get',\n",
       " 'darwin',\n",
       " 'fish',\n",
       " 'from',\n",
       " 'lynn',\n",
       " 'gold',\n",
       " 'try',\n",
       " 'mailing',\n",
       " 'figmo',\n",
       " 'netcom',\n",
       " 'com',\n",
       " 'for',\n",
       " 'net',\n",
       " 'people',\n",
       " 'who',\n",
       " 'go',\n",
       " 'to',\n",
       " 'lynn',\n",
       " 'directly',\n",
       " 'the',\n",
       " 'price',\n",
       " 'is',\n",
       " '4',\n",
       " '95',\n",
       " 'per',\n",
       " 'fish',\n",
       " 'american',\n",
       " 'atheist',\n",
       " 'press',\n",
       " 'aap',\n",
       " 'publish',\n",
       " 'various',\n",
       " 'atheist',\n",
       " 'books',\n",
       " 'critiques',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'lists',\n",
       " 'of',\n",
       " 'biblical',\n",
       " 'contradictions',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " 'one',\n",
       " 'such',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'handbook',\n",
       " 'by',\n",
       " 'w',\n",
       " 'p',\n",
       " 'ball',\n",
       " 'and',\n",
       " 'g',\n",
       " 'w',\n",
       " 'foote',\n",
       " 'american',\n",
       " 'atheist',\n",
       " 'press',\n",
       " '372',\n",
       " 'pp',\n",
       " 'isbn',\n",
       " '0',\n",
       " '910309',\n",
       " '26',\n",
       " '4',\n",
       " '2nd',\n",
       " 'edition',\n",
       " '1986',\n",
       " 'bible',\n",
       " 'contradictions',\n",
       " 'absurdities',\n",
       " 'atrocities',\n",
       " 'immoralities',\n",
       " 'contains',\n",
       " 'ball',\n",
       " 'foote',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'contradicts',\n",
       " 'itself',\n",
       " 'aap',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'king',\n",
       " 'james',\n",
       " 'version',\n",
       " 'of',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'write',\n",
       " 'to',\n",
       " 'american',\n",
       " 'atheist',\n",
       " 'press',\n",
       " 'p',\n",
       " 'o',\n",
       " 'box',\n",
       " '140195',\n",
       " 'austin',\n",
       " 'tx',\n",
       " '78714',\n",
       " '0195',\n",
       " 'or',\n",
       " '7215',\n",
       " 'cameron',\n",
       " 'road',\n",
       " 'austin',\n",
       " 'tx',\n",
       " '78752',\n",
       " '2973',\n",
       " 'telephone',\n",
       " '512',\n",
       " '458',\n",
       " '1244',\n",
       " 'fax',\n",
       " '512',\n",
       " '467',\n",
       " '9525',\n",
       " 'prometheus',\n",
       " 'books',\n",
       " 'sell',\n",
       " 'books',\n",
       " 'including',\n",
       " \"haught's\",\n",
       " 'holy',\n",
       " 'horrors',\n",
       " 'see',\n",
       " 'below',\n",
       " 'write',\n",
       " 'to',\n",
       " '700',\n",
       " 'east',\n",
       " 'amherst',\n",
       " 'street',\n",
       " 'buffalo',\n",
       " 'new',\n",
       " 'york',\n",
       " '14215',\n",
       " 'telephone',\n",
       " '716',\n",
       " '837',\n",
       " '2475',\n",
       " 'an',\n",
       " 'alternate',\n",
       " 'address',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " 'newer',\n",
       " 'or',\n",
       " 'older',\n",
       " 'is',\n",
       " 'prometheus',\n",
       " 'books',\n",
       " '59',\n",
       " 'glenn',\n",
       " 'drive',\n",
       " 'buffalo',\n",
       " 'ny',\n",
       " '14228',\n",
       " '2197',\n",
       " 'african',\n",
       " 'americans',\n",
       " 'for',\n",
       " 'humanism',\n",
       " 'an',\n",
       " 'organization',\n",
       " 'promoting',\n",
       " 'black',\n",
       " 'secular',\n",
       " 'humanism',\n",
       " 'and',\n",
       " 'uncovering',\n",
       " 'the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'black',\n",
       " 'freethought',\n",
       " 'they',\n",
       " 'publish',\n",
       " 'a',\n",
       " 'quarterly',\n",
       " 'newsletter',\n",
       " 'aah',\n",
       " 'examiner',\n",
       " 'write',\n",
       " 'to',\n",
       " 'norm',\n",
       " 'r',\n",
       " 'allen',\n",
       " 'jr',\n",
       " 'african',\n",
       " 'americans',\n",
       " 'for',\n",
       " 'humanism',\n",
       " 'p',\n",
       " 'o',\n",
       " 'box',\n",
       " '664',\n",
       " 'buffalo',\n",
       " 'ny',\n",
       " '14226',\n",
       " 'united',\n",
       " 'kingdom',\n",
       " 'rationalist',\n",
       " 'press',\n",
       " 'association',\n",
       " 'national',\n",
       " 'secular',\n",
       " 'society',\n",
       " '88',\n",
       " 'islington',\n",
       " 'high',\n",
       " 'street',\n",
       " '702',\n",
       " 'holloway',\n",
       " 'road',\n",
       " 'london',\n",
       " 'n1',\n",
       " '8ew',\n",
       " 'london',\n",
       " 'n19',\n",
       " '3nl',\n",
       " '071',\n",
       " '226',\n",
       " '7251',\n",
       " '071',\n",
       " '272',\n",
       " '1266',\n",
       " 'british',\n",
       " 'humanist',\n",
       " 'association',\n",
       " 'south',\n",
       " 'place',\n",
       " 'ethical',\n",
       " 'society',\n",
       " '14',\n",
       " \"lamb's\",\n",
       " 'conduit',\n",
       " 'passage',\n",
       " 'conway',\n",
       " 'hall',\n",
       " 'london',\n",
       " 'wc1r',\n",
       " '4rh',\n",
       " 'red',\n",
       " 'lion',\n",
       " 'square',\n",
       " '071',\n",
       " '430',\n",
       " '0908',\n",
       " 'london',\n",
       " 'wc1r',\n",
       " '4rl',\n",
       " 'fax',\n",
       " '071',\n",
       " '430',\n",
       " '1271',\n",
       " '071',\n",
       " '831',\n",
       " '7723',\n",
       " 'the',\n",
       " 'national',\n",
       " 'secular',\n",
       " 'society',\n",
       " 'publish',\n",
       " 'the',\n",
       " 'freethinker',\n",
       " 'a',\n",
       " 'monthly',\n",
       " 'magazine',\n",
       " 'founded',\n",
       " 'in',\n",
       " '1881',\n",
       " 'germany',\n",
       " 'ibka',\n",
       " 'e',\n",
       " 'v',\n",
       " 'internationaler',\n",
       " 'bund',\n",
       " 'der',\n",
       " 'konfessionslosen',\n",
       " 'und',\n",
       " 'atheisten',\n",
       " 'postfach',\n",
       " '880',\n",
       " 'd',\n",
       " '1000',\n",
       " 'berlin',\n",
       " '41',\n",
       " 'germany',\n",
       " 'ibka',\n",
       " 'publish',\n",
       " 'a',\n",
       " 'journal',\n",
       " 'miz',\n",
       " 'materialien',\n",
       " 'und',\n",
       " 'informationen',\n",
       " 'zur',\n",
       " 'zeit',\n",
       " 'politisches',\n",
       " 'journal',\n",
       " 'der',\n",
       " 'konfessionslosesn',\n",
       " 'und',\n",
       " 'atheisten',\n",
       " 'hrsg',\n",
       " 'ibka',\n",
       " 'e',\n",
       " 'v',\n",
       " 'miz',\n",
       " 'vertrieb',\n",
       " 'postfach',\n",
       " '880',\n",
       " 'd',\n",
       " '1000',\n",
       " 'berlin',\n",
       " '41',\n",
       " 'germany',\n",
       " 'for',\n",
       " 'atheist',\n",
       " 'books',\n",
       " 'write',\n",
       " 'to',\n",
       " 'ibdk',\n",
       " 'internationaler',\n",
       " 'b',\n",
       " 'ucherdienst',\n",
       " 'der',\n",
       " 'konfessionslosen',\n",
       " 'postfach',\n",
       " '3005',\n",
       " 'd',\n",
       " '3000',\n",
       " 'hannover',\n",
       " '1',\n",
       " 'germany',\n",
       " 'telephone',\n",
       " '0511',\n",
       " '211216',\n",
       " 'books',\n",
       " 'fiction',\n",
       " 'thomas',\n",
       " 'm',\n",
       " 'disch',\n",
       " 'the',\n",
       " 'santa',\n",
       " 'claus',\n",
       " 'compromise',\n",
       " 'short',\n",
       " 'story',\n",
       " 'the',\n",
       " 'ultimate',\n",
       " 'proof',\n",
       " 'that',\n",
       " 'santa',\n",
       " 'exists',\n",
       " 'all',\n",
       " 'characters',\n",
       " 'and',\n",
       " 'events',\n",
       " 'are',\n",
       " 'fictitious',\n",
       " 'any',\n",
       " 'similarity',\n",
       " 'to',\n",
       " 'living',\n",
       " 'or',\n",
       " 'dead',\n",
       " 'gods',\n",
       " 'uh',\n",
       " 'well',\n",
       " 'walter',\n",
       " 'm',\n",
       " 'miller',\n",
       " 'jr',\n",
       " 'a',\n",
       " 'canticle',\n",
       " 'for',\n",
       " 'leibowitz',\n",
       " 'one',\n",
       " 'gem',\n",
       " 'in',\n",
       " 'this',\n",
       " 'post',\n",
       " 'atomic',\n",
       " 'doomsday',\n",
       " 'novel',\n",
       " 'is',\n",
       " 'the',\n",
       " 'monks',\n",
       " 'who',\n",
       " 'spent',\n",
       " 'their',\n",
       " 'lives',\n",
       " 'copying',\n",
       " 'blueprints',\n",
       " 'from',\n",
       " 'saint',\n",
       " 'leibowitz',\n",
       " 'filling',\n",
       " 'the',\n",
       " 'sheets',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'with',\n",
       " 'ink',\n",
       " 'and',\n",
       " 'leaving',\n",
       " 'white',\n",
       " 'lines',\n",
       " 'and',\n",
       " 'letters',\n",
       " 'edgar',\n",
       " 'pangborn',\n",
       " 'davy',\n",
       " 'post',\n",
       " 'atomic',\n",
       " 'doomsday',\n",
       " 'novel',\n",
       " 'set',\n",
       " 'in',\n",
       " 'clerical',\n",
       " 'states',\n",
       " 'the',\n",
       " 'church',\n",
       " 'for',\n",
       " 'example',\n",
       " 'forbids',\n",
       " 'that',\n",
       " 'anyone',\n",
       " 'produce',\n",
       " 'describe',\n",
       " 'or',\n",
       " 'use',\n",
       " 'any',\n",
       " 'substance',\n",
       " 'containing',\n",
       " 'atoms',\n",
       " 'philip',\n",
       " 'k',\n",
       " 'dick',\n",
       " 'philip',\n",
       " 'k',\n",
       " 'dick',\n",
       " 'dick',\n",
       " 'wrote',\n",
       " 'many',\n",
       " 'philosophical',\n",
       " 'and',\n",
       " 'thought',\n",
       " 'provoking',\n",
       " 'short',\n",
       " 'stories',\n",
       " 'and',\n",
       " 'novels',\n",
       " 'his',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'bizarre',\n",
       " 'at',\n",
       " 'times',\n",
       " 'but',\n",
       " 'very',\n",
       " 'approachable',\n",
       " 'he',\n",
       " 'wrote',\n",
       " 'mainly',\n",
       " 'sf',\n",
       " 'but',\n",
       " 'he',\n",
       " 'wrote',\n",
       " 'about',\n",
       " 'people',\n",
       " 'truth',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'technology',\n",
       " 'although',\n",
       " 'he',\n",
       " 'often',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " 'met',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'god',\n",
       " 'he',\n",
       " 'remained',\n",
       " 'sceptical',\n",
       " 'amongst',\n",
       " 'his',\n",
       " 'novels',\n",
       " 'the',\n",
       " 'following',\n",
       " 'are',\n",
       " 'of',\n",
       " 'some',\n",
       " 'relevance',\n",
       " 'galactic',\n",
       " 'pot',\n",
       " 'healer',\n",
       " 'a',\n",
       " 'fallible',\n",
       " 'alien',\n",
       " 'deity',\n",
       " 'summons',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'earth',\n",
       " 'craftsmen',\n",
       " 'and',\n",
       " 'women',\n",
       " 'to',\n",
       " 'a',\n",
       " 'remote',\n",
       " 'planet',\n",
       " 'to',\n",
       " 'raise',\n",
       " 'a',\n",
       " 'giant',\n",
       " 'cathedral',\n",
       " 'from',\n",
       " 'beneath',\n",
       " 'the',\n",
       " 'oceans',\n",
       " 'when',\n",
       " 'the',\n",
       " 'deity',\n",
       " 'begins',\n",
       " 'to',\n",
       " 'demand',\n",
       " 'faith',\n",
       " 'from',\n",
       " 'the',\n",
       " 'earthers',\n",
       " 'pot',\n",
       " 'healer',\n",
       " 'joe',\n",
       " 'fernwright',\n",
       " 'is',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'comply',\n",
       " 'a',\n",
       " 'polished',\n",
       " 'ironic',\n",
       " 'and',\n",
       " 'amusing',\n",
       " 'novel',\n",
       " 'a',\n",
       " 'maze',\n",
       " 'of',\n",
       " 'death',\n",
       " 'noteworthy',\n",
       " 'for',\n",
       " 'its',\n",
       " 'description',\n",
       " 'of',\n",
       " 'a',\n",
       " 'technology',\n",
       " 'based',\n",
       " 'religion',\n",
       " 'valis',\n",
       " 'the',\n",
       " 'schizophrenic',\n",
       " 'hero',\n",
       " 'searches',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hidden',\n",
       " 'mysteries',\n",
       " 'of',\n",
       " 'gnostic',\n",
       " 'christianity',\n",
       " 'after',\n",
       " 'reality',\n",
       " 'is',\n",
       " 'fired',\n",
       " 'into',\n",
       " 'his',\n",
       " 'brain',\n",
       " 'by',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'laser',\n",
       " 'beam',\n",
       " 'of',\n",
       " 'unknown',\n",
       " 'but',\n",
       " 'possibly',\n",
       " 'divine',\n",
       " 'origin',\n",
       " 'he',\n",
       " 'is',\n",
       " 'accompanied',\n",
       " 'by',\n",
       " 'his',\n",
       " 'dogmatic',\n",
       " 'and',\n",
       " 'dismissively',\n",
       " 'atheist',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'assorted',\n",
       " 'other',\n",
       " 'odd',\n",
       " 'characters',\n",
       " 'the',\n",
       " 'divine',\n",
       " 'invasion',\n",
       " 'god',\n",
       " 'invades',\n",
       " 'earth',\n",
       " 'by',\n",
       " 'making',\n",
       " 'a',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'pregnant',\n",
       " 'as',\n",
       " 'she',\n",
       " 'returns',\n",
       " 'from',\n",
       " 'another',\n",
       " 'star',\n",
       " 'system',\n",
       " 'unfortunately',\n",
       " 'she',\n",
       " 'is',\n",
       " 'terminally',\n",
       " 'ill',\n",
       " 'and',\n",
       " 'must',\n",
       " 'be',\n",
       " 'assisted',\n",
       " 'by',\n",
       " 'a',\n",
       " 'dead',\n",
       " 'man',\n",
       " 'whose',\n",
       " 'brain',\n",
       " 'is',\n",
       " 'wired',\n",
       " 'to',\n",
       " '24',\n",
       " 'hour',\n",
       " 'easy',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'margaret',\n",
       " 'atwood',\n",
       " 'the',\n",
       " \"handmaid's\",\n",
       " 'tale',\n",
       " 'a',\n",
       " 'story',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'premise',\n",
       " 'that',\n",
       " 'the',\n",
       " 'us',\n",
       " 'congress',\n",
       " 'is',\n",
       " 'mysteriously',\n",
       " 'assassinated',\n",
       " 'and',\n",
       " 'fundamentalists',\n",
       " 'quickly',\n",
       " 'take',\n",
       " 'charge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'nation',\n",
       " 'to',\n",
       " 'set',\n",
       " 'it',\n",
       " 'right',\n",
       " 'again',\n",
       " 'the',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'diary',\n",
       " 'of',\n",
       " 'a',\n",
       " \"woman's\",\n",
       " 'life',\n",
       " 'as',\n",
       " 'she',\n",
       " 'tries',\n",
       " 'to',\n",
       " 'live',\n",
       " 'under',\n",
       " 'the',\n",
       " 'new',\n",
       " 'christian',\n",
       " 'theocracy',\n",
       " \"women's\",\n",
       " 'right',\n",
       " 'to',\n",
       " 'own',\n",
       " 'property',\n",
       " 'is',\n",
       " 'revoked',\n",
       " 'and',\n",
       " 'their',\n",
       " 'bank',\n",
       " 'accounts',\n",
       " 'are',\n",
       " 'closed',\n",
       " 'sinful',\n",
       " 'luxuries',\n",
       " 'are',\n",
       " 'outlawed',\n",
       " 'and',\n",
       " 'the',\n",
       " 'radio',\n",
       " 'is',\n",
       " 'only',\n",
       " 'used',\n",
       " 'for',\n",
       " 'readings',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'crimes',\n",
       " 'are',\n",
       " 'punished',\n",
       " 'retroactively',\n",
       " 'doctors',\n",
       " 'who',\n",
       " 'performed',\n",
       " 'legal',\n",
       " 'abortions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'old',\n",
       " 'world',\n",
       " 'are',\n",
       " 'hunted',\n",
       " 'down',\n",
       " 'and',\n",
       " 'hanged',\n",
       " \"atwood's\",\n",
       " 'writing',\n",
       " 'style',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'get',\n",
       " 'used',\n",
       " 'to',\n",
       " 'at',\n",
       " 'first',\n",
       " 'but',\n",
       " 'the',\n",
       " 'tale',\n",
       " 'grows',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'chilling',\n",
       " 'as',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'various',\n",
       " 'authors',\n",
       " 'the',\n",
       " 'bible',\n",
       " 'this',\n",
       " 'somewhat',\n",
       " 'dull',\n",
       " 'and',\n",
       " 'rambling',\n",
       " 'work',\n",
       " 'has',\n",
       " 'often',\n",
       " 'been',\n",
       " 'criticized',\n",
       " 'however',\n",
       " 'it',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'worth',\n",
       " 'reading',\n",
       " 'if',\n",
       " 'only',\n",
       " 'so',\n",
       " 'that',\n",
       " \"you'll\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'all',\n",
       " 'the',\n",
       " 'fuss',\n",
       " 'is',\n",
       " 'about',\n",
       " 'it',\n",
       " 'exists',\n",
       " 'in',\n",
       " 'many',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'so',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'one',\n",
       " 'true',\n",
       " 'version',\n",
       " 'books',\n",
       " 'non',\n",
       " 'fiction',\n",
       " 'peter',\n",
       " 'de',\n",
       " 'rosa',\n",
       " 'vicars',\n",
       " 'of',\n",
       " 'christ',\n",
       " 'bantam',\n",
       " 'press',\n",
       " '1988',\n",
       " 'although',\n",
       " 'de',\n",
       " 'rosa',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'be',\n",
       " 'christian',\n",
       " 'or',\n",
       " 'even',\n",
       " 'catholic',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'enlighting',\n",
       " 'history',\n",
       " 'of',\n",
       " 'papal',\n",
       " 'immoralities',\n",
       " 'adulteries',\n",
       " 'fallacies',\n",
       " 'etc',\n",
       " 'german',\n",
       " 'translation',\n",
       " 'gottes',\n",
       " 'erste',\n",
       " 'diener',\n",
       " 'die',\n",
       " 'dunkle',\n",
       " 'seite',\n",
       " 'des',\n",
       " 'papsttums',\n",
       " 'droemer',\n",
       " 'knaur',\n",
       " '1989',\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "for doc in text:\n",
    "a    doc_vector = []\n",
    "    seq = text_to_word_sequence(texts[0])\n",
    "    for word in seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "    word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x_2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x_3 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "f = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(x_1)\n",
    "s = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(x_2)\n",
    "t = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)(x_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Conv1D(filters=200,\n",
    "                     kernel_size = 4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(f)\n",
    "f = GlobalMaxPooling1D()(f)\n",
    "\n",
    "s = Conv1D(filters=200,\n",
    "                     kernel_size = 4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(s)\n",
    "s = GlobalMaxPooling1D()(s)\n",
    "\n",
    "\n",
    "t = Conv1D(filters=200,\n",
    "                     kernel_size = 4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(t)\n",
    "t = GlobalMaxPooling1D()(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = Dense(units=10, activation='relu')(Concatenate()([f,s]))\n",
    "st = Dense(units=10, activation='relu')(Concatenate()([s,t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_layer = Concatenate()([fs, f, s, t, st])\n",
    "hidden = Dense(units=400, activation='relu')(join_layer)\n",
    "hidden = Dropout(0.2)(hidden)\n",
    "is_coherrent = Dense(units=1, activation='sigmoid')(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[x_1,x_2,x_3], outputs=is_coherrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 1000, 50)     20000050    input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 1000, 50)     20000050    input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1000, 50)     20000050    input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 997, 200)     40200       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 997, 200)     40200       embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 997, 200)     40200       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_136 (Globa (None, 200)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_137 (Globa (None, 200)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_138 (Globa (None, 200)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 400)          0           global_max_pooling1d_136[0][0]   \n",
      "                                                                 global_max_pooling1d_137[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 400)          0           global_max_pooling1d_137[0][0]   \n",
      "                                                                 global_max_pooling1d_138[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 10)           4010        concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 10)           4010        concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_53 (Concatenate)    (None, 620)          0           dense_46[0][0]                   \n",
      "                                                                 global_max_pooling1d_136[0][0]   \n",
      "                                                                 global_max_pooling1d_137[0][0]   \n",
      "                                                                 global_max_pooling1d_138[0][0]   \n",
      "                                                                 dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 400)          248400      concatenate_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 400)          0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 1)            401         dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 60,377,571\n",
      "Trainable params: 377,421\n",
      "Non-trainable params: 60,000,150\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizers' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-ebbf6fd087fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madadelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Use map_score as a metric here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.compile(optimizer=adadelta,\n\u001b[1;32m      4\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizers' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "adadelta = optimizers.Adadelta(rho=0.95, epsilon=1e-06)\n",
    "# TODO: Use map_score as a metric here?\n",
    "model.compile(optimizer=adadelta,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityLayer(Layer):\n",
    "\n",
    "    def __init__(self,dim=200, **kwargs):\n",
    "        self.dim = dim\n",
    "        super(SimilarityLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(self.dim, self.dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(SimilarityLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(K.dot(x[0][:self.dim], self.kernel),x[0][self.dim:])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_conv(embedding_layer,filters,kernel_size):\n",
    "    conv_model = Sequential()\n",
    "    conv_model.add(embedding_layer)\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    conv_model.add(Conv1D(filters=200,\n",
    "                     kernel_size = 4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    conv_model.add(GlobalMaxPooling1D())\n",
    "    print(conv_model.summary())\n",
    "    return conv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 997, 200)          40200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_127 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,250\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 997, 200)          40200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_128 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,250\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 997, 200)          40200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_129 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,250\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "conv_f = get_embedded_conv(embedding_layer,250,4)\n",
    "conv_s = get_embedded_conv(embedding_layer,250,4)\n",
    "conv_t = get_embedded_conv(embedding_layer,250,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-28501b3f1f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;31m# know about its input shape. Otherwise, that's an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_input_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                         raise ValueError('The first layer in a '\n\u001b[0m\u001b[1;32m    485\u001b[0m                                          \u001b[0;34m'Sequential model must '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                                          \u001b[0;34m'get an `input_shape` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "fs = Sequential()\n",
    "fs.add(Merge([conv_f, conv_s],mode='concat'))\n",
    "fs.add(Dense(10, activation='relu'))\n",
    "\n",
    "st = Sequential()\n",
    "st.add(Merge([conv_s, conv_t],mode='concat'))\n",
    "st.add(Dense(10, activation='relu'))\n",
    "\n",
    "model =Sequential()\n",
    "model.add(Concatenate([fs, conv_f, conv_s, conv_t, st]))\n",
    "model.add(Dense(units=400, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 997, 200)          40200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_126 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,250\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fe64898b048>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedded_conv(embedding_layer,250,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", name=\"Convolution-2gram\", filters=200, kernel_size=2, padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", name=\"Convolution-3gram\", filters=200, kernel_size=3, padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", name=\"Convolution-4gram\", filters=200, kernel_size=4, padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"tanh\", name=\"Convolution-5gram\", filters=200, kernel_size=5, padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "Convolution-2gram (Conv1D)   (None, 1000, 200)         20200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_117 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,020,250\n",
      "Trainable params: 20,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "Convolution-3gram (Conv1D)   (None, 1000, 200)         30200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_118 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,030,250\n",
      "Trainable params: 30,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "Convolution-4gram (Conv1D)   (None, 1000, 200)         40200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_119 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,250\n",
      "Trainable params: 40,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "Convolution-5gram (Conv1D)   (None, 1000, 200)         50200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_120 (Gl (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 20,050,250\n",
      "Trainable params: 50,200\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_7 (Merge)              (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 401       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 20,461,651\n",
      "Trainable params: 461,601\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ngram_filters = [2, 3, 4, 5]\n",
    "conv_hidden_units = [200, 200, 200, 200]\n",
    "convsM = []\n",
    "for n_gram, hidden_units in zip(ngram_filters, conv_hidden_units):\n",
    "    conv_ = Sequential()\n",
    "    conv_.add(embedding_layer)\n",
    "    conv_.add(Convolution1D(nb_filter=hidden_units,\n",
    "                             filter_length=n_gram,\n",
    "                             border_mode='same',\n",
    "                             activation='tanh', name='Convolution-'+str(n_gram)+\"gram\"))\n",
    "    \n",
    "    conv_.add(GlobalMaxPooling1D())\n",
    "    print(conv_.summary())\n",
    "    convsM.append(conv_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge(convsM, mode='concat'))\n",
    "model.add(Dense(units=400, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "# conv =concatenate(inputs=convsM,axis=1)\n",
    "# model.add( Dense(128, activation='relu')(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_f ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 397, 250)          50250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 363,251\n",
      "Trainable params: 363,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from model import *\n",
    "\n",
    "DATASET_DIR = 'data/cui/processed/'\n",
    "\n",
    "\n",
    "def pad_sequences_(sequences):\n",
    "    ans = []\n",
    "    for sequence in sequences:\n",
    "        ans.extend(pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH))\n",
    "    return ans\n",
    "\n",
    "def pad_all(dict_):\n",
    "    \"\"\"\n",
    "\n",
    "    :param dict:\n",
    "    :return: 0 -> [   0 -> first sentence\n",
    "                     1 -> second sentence\n",
    "                     2 -> third sentence\n",
    "                 ],\n",
    "             1 -> label\n",
    "    \"\"\"\n",
    "    x_1 = dict_['firsts']\n",
    "    x_2 = dict_['seconds']\n",
    "    x_3 = dict_['thirds']\n",
    "    \n",
    "#     y = []\n",
    "#     for i in range(len(dict_['firsts'])):\n",
    "#         y.extend([dict_['labels'][i] for j in range(len(dict_['firsts'][i]))])\n",
    "    x_1 = pad_sequences(x_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    x_2 = pad_sequences(x_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    x_3 = pad_sequences(x_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        \n",
    "    return [[x_1, x_2, x_3],dict_['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading train\")\n",
    "# with open(DATASET_DIR + \"train.json\") as f:\n",
    "#     train = json.load(f)\n",
    "#     print(\"Loaded train\")\n",
    "# #\n",
    "# print(\"Loading test\")\n",
    "# with open(DATASET_DIR + \"test.json\") as f:\n",
    "#     test = pad_all(json.load(f))\n",
    "\n",
    "# print(\"Loading dev\")\n",
    "\n",
    "with open(DATASET_DIR + \"dev.json\") as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev['thirds'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_all(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0, 1911, 4868,\n",
       "       1968, 1534], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train['firsts']\n",
    "y = train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20580"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20580"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5918, 1110],\n",
       " [1993, 4868, 2404, 1110, 1968],\n",
       " [2159, 5918, 1110],\n",
       " [1864, 4868, 3814, 2159, 1911, 4868, 5025],\n",
       " [1534, 2159, 41, 1864, 4652],\n",
       " [3880, 4868, 1911, 5140, 7, 1911, 1968],\n",
       " [2159, 4868],\n",
       " [41, 3814, 1864, 1911, 1110, 7, 1534, 1110],\n",
       " [7, 41, 1911, 1534, 3420, 1110, 1110, 1968],\n",
       " [1],\n",
       " [7, 3814, 1968],\n",
       " [2159, 5918, 1110],\n",
       " [1534, 5918, 6479, 2159, 2159, 1110, 1911],\n",
       " [1864, 4868, 3814, 2159, 41, 3814, 6479, 1110, 1968],\n",
       " [2]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
